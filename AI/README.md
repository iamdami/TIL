Ref.
===

- [use float number as opencv image pixel coordinates](https://stackoverflow.com/questions/58991754/how-to-use-float-number-as-opencv-image-pixel-coordinates)  
- [파이썬 del / remove()](https://ooyoung.tistory.com/49)
- [cv2.dnn.blobFromImage](https://www.inflearn.com/questions/29011)
- [Objective Function, Loss Function, Cost Function](https://ganghee-lee.tistory.com/28)
- [인공지능의 결정적 순간들](https://www.letr.ai/explore/story-20211105-1)
- [인공지능](https://gracefulprograming.tistory.com/99)
- [신경망, 지도학습](https://damio.tistory.com/1?category=1172751)
- [인공신경망 (Artificial Neural Network, ANN)과 역전파 알고리즘 (Backpropagation Algorithm)](https://untitledtblog.tistory.com/141)
- [인공신경망](https://brunch.co.kr/@gdhan/6)
- [경사소실문제](http://cbjsena.blogspot.com/2018/12/blog-post_25.html)
- [로지스틱 회귀 비용함수](https://damio.tistory.com/6)
- [경사하강법](https://damio.tistory.com/7)
- [나이브 베이지안 알고리즘](https://namu.wiki/w/%EB%82%98%EC%9D%B4%EB%B8%8C%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)
- [퍼지 이론을 활용한 주유소 추천 방법](https://www.mrlatte.net/blog/2019/12/29/gas-station-recommendation-method-using-fuzzy.html)  
- [인공지능 연구의 두가지 큰 흐름, 기호주의와 연결주의](https://6u2ni.tistory.com/36)  
- [오차 역전파, 경사하강법](https://sacko.tistory.com/19)  
- [퍼셉트론의 개념 (단층 퍼셉트론 / 다층 퍼셉트론)](https://0-sunny.tistory.com/72)  
- [회귀 모델 종류와 특징](https://brunch.co.kr/@gimmesilver/38)  
- [머신러닝 분류 알고리즘](https://iphoong.tistory.com/6)  
- [RetinaNet 논문(Focal Loss for Dense Object Detection) 리뷰](https://herbwood.tistory.com/19)  
- [목적함수 손실함수 비용함수 차이](https://velog.io/@eunice123/%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98-%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98-%EC%B0%A8%EC%9D%B4)  
- [(CS231N) Lecture3](https://sangminwoo.github.io/2019-02-13-cs231n-lecture3/)  
- [python opencv 카메라 출력](https://076923.github.io/posts/Python-opencv-2/)  
- [활성화 함수](https://blog.naver.com/jaeyoon_95/222300238922)  
- [베이즈 정리](https://angeloyeo.github.io/2020/01/09/Bayes_rule.html)  
- [최적화 함수](https://blog.naver.com/jaeyoon_95/222309856345)  
- [퍼지 이론](https://happy8earth.tistory.com/501)  
- [독립변수 종속변수](https://jesuisjavert.github.io/2021/01/06/machinelearning-python3/)  
- [군집화, 분류 구분](https://jjeongil.tistory.com/389)  
- [군집화 유사도 척도 - 거리 측도 방식](https://yu1moo.tistory.com/entry/%EA%B5%B0%EC%A7%91%ED%99%94-Clustering)  
- [opencv 비디오 읽기](https://zzsza.github.io/data/2018/01/23/opencv-1/)  
- [딥러닝 활성화함수 - 소프트맥스](https://gooopy.tistory.com/53)  
- [One Hot Encoding](https://wikidocs.net/22647)  
- [파라미터와 하이퍼파라미터](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-13-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0Parameter%EC%99%80-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0Hyper-parameter)  
- [신경망 학습 최적화](https://sacko.tistory.com/42)  
- [Optimization Algorithm](https://velog.io/@minjung-s/Optimization-Algorithm)  
- [Optimization 최적화 기법](https://my-coding-footprints.tistory.com/101)  
- [softmax classification](https://at0z.tistory.com/31)  
- [Integer Encoding](https://velog.io/@ganta/%EC%A0%95%EC%88%98-%EC%9D%B8%EC%BD%94%EB%94%A9Integer-Encoding)  
- [Optimizer](https://gomguard.tistory.com/187)  
- [Python, OpenCV 얼굴인식](https://velog.io/@sidcode/Python-OpenCV-%EC%96%BC%EA%B5%B4%EC%9D%B8%EC%8B%9D)  
- [Optimization for Deep Learning](https://sonsnotation.blogspot.com/2020/11/6-optimization-for-deep-learning.html)  
- [Generalization, Optimization](https://yongku.tistory.com/entry/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9D%BC%EB%B0%98%ED%99%94Generalization%EC%99%80-%EC%B5%9C%EC%A0%81%ED%99%94Optimization)  
- [Optimizer](https://heeya-stupidbutstudying.tistory.com/37)  
- [What are hyperparameters in Neural Networks and what it means to tune them?](https://www.quora.com/What-are-hyperparameters-in-Neural-Networks-and-what-it-means-to-tune-them)  
- [Loss Function vs Cost Function vs Objective Function](https://ggodong.tistory.com/15)  
- [비용함수, 손실함수, 목적함수](https://velog.io/@regista/%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98Cost-Function-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98Loss-function-%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98Objective-Function-Ai-tech)  
- [Loss Function](https://needjarvis.tistory.com/567)  
- [Loss function에 대해](https://blog.naver.com/PostView.naver?blogId=vail131&logNo=222476383104)  
- [Linear Regression - 선형회귀의 평가 지표, MAE, MSE, RMSE, R Squared](https://velog.io/@dlskawns/Linear-Regression-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%9D%98-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-MAE-MSE-RMSE-R-Squared-%EC%A0%95%EB%A6%AC)  
- [Cross-entropy 의 이해: 정보이론과의 관계](https://3months.tistory.com/436)  
- [왜 크로스 엔트로피를 쓸까?](https://theeluwin.postype.com/post/6080524)  
- [손실함수 (loss function) 종류 및 정리](https://didu-story.tistory.com/27)  
- [Linear Regression](https://ko.d2l.ai/chapter_deep-learning-basics/linear-regression.html)  
- [Sigmoid Softmax](https://feel0804.tistory.com/6)  
- [활성화 함수 사용 이유](https://ganghee-lee.tistory.com/30)  
- [활성화 함수 종류 및 비교](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339)  
- [Activation Function: Zero-centered](https://sykflyinginthesky.tistory.com/8)  
- [옵티마이저 종류 및 정리](https://ganghee-lee.tistory.com/24)  
- [cv2 image tp pil image](https://bskyvision.com/1170)  
- [cosine similarity](https://bkshin.tistory.com/entry/NLP-8-%EB%AC%B8%EC%84%9C-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84)  
- [classification](https://yjoo0913.tistory.com/12)  
- [Gradient Descent 세 종류](https://velog.io/@crosstar1228/MLGradient-Descent-%EC%9D%98-%EC%84%B8-%EC%A2%85%EB%A5%98Batch-Stochastic-Mini-Batch)  
- [Full batch, mini-batch, and online learning](https://www.kaggle.com/residentmario/full-batch-mini-batch-and-online-learning/notebook)  
![image](https://user-images.githubusercontent.com/50016477/158554853-901f508d-20ec-4c01-8979-0f8ccfd0d79c.png)  
- [오차역전파법](https://amber-chaeeunk.tistory.com/18)  
- [경사하강법](https://velog.io/@zeen263/Week-2-2-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95)  
- [경사하강법](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)  
- [활성화 함수](https://wikidocs.net/60683)  
  - 활성화 함수는 비선형 함수를 써야 함(계단함수도 비선형)  
- [Softmax function](https://syj9700.tistory.com/38)  
- [Sigmoid, Logit and Softmax](https://chacha95.github.io/2019-04-04-logit/)  
  - 딥러닝 모델의 마지막 노드들에 출력 값 바꿀 때 logit 함수와 softmax 쓰는 이유  
- [딥러닝 신경망 구현 - 출력층, 항등함수, 소프트맥스, 분류, 회귀](https://yaneodoo2.tistory.com/entry/05-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EA%B5%AC%ED%98%84-%EA%B8%B0%EC%B4%88%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83)  
  - 회귀에서 항등함수 쓰는 이유  
- [Activation Function](https://junstar92.tistory.com/122)  
  - 활성화함수에 비선형함수 써야하는 이유  
