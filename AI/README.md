Ref.
===

- [use float number as opencv image pixel coordinates](https://stackoverflow.com/questions/58991754/how-to-use-float-number-as-opencv-image-pixel-coordinates)  
- [파이썬 del / remove()](https://ooyoung.tistory.com/49)
- [cv2.dnn.blobFromImage](https://www.inflearn.com/questions/29011)
- [Objective Function, Loss Function, Cost Function](https://ganghee-lee.tistory.com/28)
- [인공지능의 결정적 순간들](https://www.letr.ai/explore/story-20211105-1)
- [인공지능](https://gracefulprograming.tistory.com/99)
- [신경망, 지도학습](https://damio.tistory.com/1?category=1172751)
- [인공신경망 (Artificial Neural Network, ANN)과 역전파 알고리즘 (Backpropagation Algorithm)](https://untitledtblog.tistory.com/141)
- [인공신경망](https://brunch.co.kr/@gdhan/6)
- [경사소실문제](http://cbjsena.blogspot.com/2018/12/blog-post_25.html)
- [로지스틱 회귀 비용함수](https://damio.tistory.com/6)
- [경사하강법](https://damio.tistory.com/7)
- [나이브 베이지안 알고리즘](https://namu.wiki/w/%EB%82%98%EC%9D%B4%EB%B8%8C%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)
- [퍼지 이론을 활용한 주유소 추천 방법](https://www.mrlatte.net/blog/2019/12/29/gas-station-recommendation-method-using-fuzzy.html)  
- [인공지능 연구의 두가지 큰 흐름, 기호주의와 연결주의](https://6u2ni.tistory.com/36)  
- [오차 역전파, 경사하강법](https://sacko.tistory.com/19)  
- [퍼셉트론의 개념 (단층 퍼셉트론 / 다층 퍼셉트론)](https://0-sunny.tistory.com/72)  
- [회귀 모델 종류와 특징](https://brunch.co.kr/@gimmesilver/38)  
- [머신러닝 분류 알고리즘](https://iphoong.tistory.com/6)  
- [RetinaNet 논문(Focal Loss for Dense Object Detection) 리뷰](https://herbwood.tistory.com/19)  
- [목적함수 손실함수 비용함수 차이](https://velog.io/@eunice123/%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98-%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98-%EC%B0%A8%EC%9D%B4)  
- [(CS231N) Lecture3](https://sangminwoo.github.io/2019-02-13-cs231n-lecture3/)  
- [python opencv 카메라 출력](https://076923.github.io/posts/Python-opencv-2/)  
- [활성화 함수](https://blog.naver.com/jaeyoon_95/222300238922)  
- [베이즈 정리](https://angeloyeo.github.io/2020/01/09/Bayes_rule.html)  
- [최적화 함수](https://blog.naver.com/jaeyoon_95/222309856345)  
- [퍼지 이론](https://happy8earth.tistory.com/501)  
- [독립변수 종속변수](https://jesuisjavert.github.io/2021/01/06/machinelearning-python3/)  
- [군집화, 분류 구분](https://jjeongil.tistory.com/389)  
- [군집화 유사도 척도 - 거리 측도 방식](https://yu1moo.tistory.com/entry/%EA%B5%B0%EC%A7%91%ED%99%94-Clustering)  
- [opencv 비디오 읽기](https://zzsza.github.io/data/2018/01/23/opencv-1/)  
- [딥러닝 활성화함수 - 소프트맥스](https://gooopy.tistory.com/53)  
- [One Hot Encoding](https://wikidocs.net/22647)  
- [파라미터와 하이퍼파라미터](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-13-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0Parameter%EC%99%80-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0Hyper-parameter)  
- [신경망 학습 최적화](https://sacko.tistory.com/42)  
- [Optimization Algorithm](https://velog.io/@minjung-s/Optimization-Algorithm)  
- [Optimization 최적화 기법](https://my-coding-footprints.tistory.com/101)  
- [softmax classification](https://at0z.tistory.com/31)  
- [Integer Encoding](https://velog.io/@ganta/%EC%A0%95%EC%88%98-%EC%9D%B8%EC%BD%94%EB%94%A9Integer-Encoding)  
- [Optimizer](https://gomguard.tistory.com/187)  
- [Python, OpenCV 얼굴인식](https://velog.io/@sidcode/Python-OpenCV-%EC%96%BC%EA%B5%B4%EC%9D%B8%EC%8B%9D)  
- [Optimization for Deep Learning](https://sonsnotation.blogspot.com/2020/11/6-optimization-for-deep-learning.html)  
- [Generalization, Optimization](https://yongku.tistory.com/entry/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9D%BC%EB%B0%98%ED%99%94Generalization%EC%99%80-%EC%B5%9C%EC%A0%81%ED%99%94Optimization)  
- [Optimizer](https://heeya-stupidbutstudying.tistory.com/37)  
- [What are hyperparameters in Neural Networks and what it means to tune them?](https://www.quora.com/What-are-hyperparameters-in-Neural-Networks-and-what-it-means-to-tune-them)  
- [Loss Function vs Cost Function vs Objective Function](https://ggodong.tistory.com/15)  
- [비용함수, 손실함수, 목적함수](https://velog.io/@regista/%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98Cost-Function-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98Loss-function-%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98Objective-Function-Ai-tech)  
- [Loss Function](https://needjarvis.tistory.com/567)  
- [Loss function에 대해](https://blog.naver.com/PostView.naver?blogId=vail131&logNo=222476383104)  
- [Linear Regression - 선형회귀의 평가 지표, MAE, MSE, RMSE, R Squared](https://velog.io/@dlskawns/Linear-Regression-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%9D%98-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-MAE-MSE-RMSE-R-Squared-%EC%A0%95%EB%A6%AC)  
- [Cross-entropy 의 이해: 정보이론과의 관계](https://3months.tistory.com/436)  
- [왜 크로스 엔트로피를 쓸까?](https://theeluwin.postype.com/post/6080524)  
- [손실함수 (loss function) 종류 및 정리](https://didu-story.tistory.com/27)  
- [Linear Regression](https://ko.d2l.ai/chapter_deep-learning-basics/linear-regression.html)  
- [Sigmoid Softmax](https://feel0804.tistory.com/6)  
- [활성화 함수 사용 이유](https://ganghee-lee.tistory.com/30)  
- [활성화 함수 종류 및 비교](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339)  
- [Activation Function: Zero-centered](https://sykflyinginthesky.tistory.com/8)  
- [옵티마이저 종류 및 정리](https://ganghee-lee.tistory.com/24)  
- [cv2 image tp pil image](https://bskyvision.com/1170)  
- [cosine similarity](https://bkshin.tistory.com/entry/NLP-8-%EB%AC%B8%EC%84%9C-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84)  
- [classification](https://yjoo0913.tistory.com/12)  
- [Gradient Descent 세 종류](https://velog.io/@crosstar1228/MLGradient-Descent-%EC%9D%98-%EC%84%B8-%EC%A2%85%EB%A5%98Batch-Stochastic-Mini-Batch)  
- [Full batch, mini-batch, and online learning](https://www.kaggle.com/residentmario/full-batch-mini-batch-and-online-learning/notebook)  
![image](https://user-images.githubusercontent.com/50016477/158554853-901f508d-20ec-4c01-8979-0f8ccfd0d79c.png)  
- [오차역전파법](https://amber-chaeeunk.tistory.com/18)  
- [경사하강법](https://velog.io/@zeen263/Week-2-2-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95)  
- [경사하강법](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)  
- [활성화 함수](https://wikidocs.net/60683)  
  - 활성화 함수는 비선형 함수를 써야 함(계단함수도 비선형)  
- [Softmax function](https://syj9700.tistory.com/38)  
- [Sigmoid, Logit and Softmax](https://chacha95.github.io/2019-04-04-logit/)  
  - 딥러닝 모델의 마지막 노드들에 출력 값 바꿀 때 logit 함수와 softmax 쓰는 이유  
- [딥러닝 신경망 구현 - 출력층, 항등함수, 소프트맥스, 분류, 회귀](https://yaneodoo2.tistory.com/entry/05-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EA%B5%AC%ED%98%84-%EA%B8%B0%EC%B4%88%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83)  
  - 회귀에서 항등함수 쓰는 이유  
- [Activation Function](https://junstar92.tistory.com/122)  
  - 활성화함수에 비선형함수 써야하는 이유  
- [Convolution and Pooling](https://wikidocs.net/62306)  
- [Batch Normalization](https://eehoeskrap.tistory.com/430)  
- [배치 정규화](https://sacko.tistory.com/44)  
- [Batch Normalization](http://funmv2013.blogspot.com/2016/09/batch-normalization.html)  
- [CNN, Convolutional Neural Network 요약](http://taewan.kim/post/cnn/)  
- [IoU(Intersection over Union)](https://deep-learning-study.tistory.com/402)  
  - object detector의 정확도를 측정하는데 이용되는 평가 지표  
- [CNN - pooling, flatten](https://hwanine.github.io/ai/CNN2/)  
- [CNN 정리 잘 되어있는 블로그1](https://dhhwang89.tistory.com/89)  
- [CNN 정리 잘 되어있는 블로그2](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)  
- [Fully Connected vs Convolutional Neural Networks](https://medium.com/swlh/fully-connected-vs-convolutional-neural-networks-813ca7bc6ee5)  
![image](https://user-images.githubusercontent.com/50016477/159254652-43024cf5-22a8-4bfa-bf38-65f0f184898d.png)
![image](https://user-images.githubusercontent.com/50016477/159254725-7ee019e7-f924-44a8-b472-941a2f22f21b.png)
- [fully connected nn](https://deeplearningmath.org/general-fully-connected-neural-networks.html)  
- [Fully-Connected Neural Network Layer](https://www.gabormelli.com/RKB/Fully-Connected_Neural_Network_Layer)  
- [deep convolutional networks](http://neuralnetworksanddeeplearning.com/chap6.html)  
- [규제화(Regularization)](https://gilbertlim.github.io/machine%20learning/ml_regulation/)  
  - model이 train data에 너무 학습되지 않도록 함  
- [3*3 convolution filters and one 5*5 convolution filters](https://stackoverflow.com/questions/51015834/in-deep-learning-whats-difference-between-two-33-convolution-filters-and-one-5)  
  - two 3*3 convolution filter will make network more deep and extract more complex features than one 5*5 convolution filter  
- [best convolution filter size](https://nittaku.tistory.com/266)  
- [Non-Maximum Suppression](https://naknaklee.github.io/etc/2021/03/08/NMS/)  
  - object detectior가 예측한 bounding box중에서 정확한 bounding box를 선택하도록 하는 기법  
- [NMS이해, PyTorch 구현](https://deep-learning-study.tistory.com/403)  
- [CNN 자세히 정리된 블로그](https://data-scientist-brian-kim.tistory.com/86)  
- [CNN BottleNeck](https://coding-yoon.tistory.com/116)  
  - 복잡도 증가시키지 않기 위해 효과적으로 사용  
  - 작은 kernel 사용함으로서 parameter 수 감소시킴  
- [Does MaxPooling reduce overfitting?](https://stackoverflow.com/questions/59717290/does-maxpooling-reduce-overfitting)  
  - 응 절대 아니지~  
  <pre>DONT use max pooling for the purpose of reducing overfitting 
  because it's is used to reduce the rapresentation 
  and to make the network a bit more robust to some features, 
  further more using it so much will make the network more and more robust 
  to a some kind of featuers.</pre>  
- [stochastic pooling](https://blog.naver.com/laonple/220830178487)  
  - max pooling과 average pooling의 문제점 해결 위해 고안된 방법  
  - 단순히 activation 선택하거나 모든 activation 평균 구하는 게 아니라 drop out과 마찬가지로 확률 p에 따라 적절한 activation 선택  
- [pooling layer](https://kevinthegrey.tistory.com/142)  
- [Keras - pooling layers](https://keras.io/api/layers/pooling_layers/)  
- [padding](https://ardino.tistory.com/40)  
  - valid padding: padding 추가하지 않은 형태
    valid padding 적용 후 필터 통과시키면 그 결과는 항상 입력 사이즈보다 작게 됨  
  - full padding: 입력 데이터의 모든 원소가 합성곱 연산에 같은 비율로 참여하도록 하는 형태
  - same padding(half padding): 출력 크기를 입력 크기와 동일하게 유지하는 형태  
- [overfitting 해결 - Regularization(규제화)](https://warm-uk.tistory.com/51)  
- [Dropout Regularization (C2W1L06)](https://www.youtube.com/watch?v=D8PJAL-MZv8)  
- [](https://tutorials.pytorch.kr/beginner/blitz/neural_networks_tutorial.html)  
  - nn은 모델 정의하고 미분하는데 autograd 사용  
- https://pytorch.org/docs/stable/nn.functional.html#convolution-functions  
  - pytorch Convolution functions  
- [Pooling Layers for Convolutional Neural Networks](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)  
  - Pooling is required to down sample the detection of features in feature maps.  
  - How to calculate and implement average and maximum pooling in a convolutional neural network.  
  - How to use global pooling in a convolutional neural network.  
- [Source code for bob.learn.pytorch.architectures.LightCNN](https://www.idiap.ch/software/bob/docs/bob/bob.learn.pytorch/stable/_modules/bob/learn/pytorch/architectures/LightCNN.html)  
  - https://github.com/AlfredXiangWu/LightCNN 찾았어요 선생님...
